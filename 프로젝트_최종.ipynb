{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 대구 교통사고 피해 예측 AI 경진대회(https://dacon.io/competitions/official/236193/overview/description)\n",
        "## 목표: 다양한 데이터들을 통해 test의 ECLO 예측\n",
        "ECLO(Equivalent Casualty Loss Only) : 인명피해 심각도  \n",
        "ECLO = 사망자수 * 10 + 중상자수 * 5 + 경상자수 * 3 + 부상자수 * 1  \n",
        "본 대회에서는 사고의 위험도를 인명피해 심각도로 측정"
      ],
      "metadata": {
        "id": "NMNe8p-Yfwsv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyW-W0E45Go9",
        "outputId": "b6804ee8-8f4f-43c2-dfaf-3675b2827e63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.5-cp310-cp310-manylinux2014_x86_64.whl (98.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.11.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.3.0)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.2.5\n"
          ]
        }
      ],
      "source": [
        "%pip install catboost"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 라이브러리"
      ],
      "metadata": {
        "id": "gvrfm5KIf2mp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "O6PsvMqY5Gm6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
        "from scipy.stats import chi2_contingency\n",
        "from scipy.stats import pearsonr\n",
        "import random\n",
        "import os\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.model_selection import StratifiedKFold"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 시드 고정"
      ],
      "metadata": {
        "id": "d-B83mRBf6ey"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fOFQig8J5GiA"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "seed_everything(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 레이블 인코더 변수 생성"
      ],
      "metadata": {
        "id": "ZKGDSBMwgAXW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ydd-73f55Gfi"
      },
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 전국구 데이터 가져오기"
      ],
      "metadata": {
        "id": "ePQN7Yayf9jY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "c5fMclTT5GdF",
        "outputId": "f7ed91f9-4639-477c-aede-12ab496b20b8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './countrywide_accident.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-47777b584f9c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcountrywide\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./countrywide_accident.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# '사고일시' 컬럼을 날짜 형식으로 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcountrywide\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'사고일시'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountrywide\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'사고일시'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './countrywide_accident.csv'"
          ]
        }
      ],
      "source": [
        "countrywide = pd.read_csv('./countrywide_accident.csv')\n",
        "\n",
        "# '사고일시' 컬럼을 날짜 형식으로 변환\n",
        "countrywide['사고일시'] = pd.to_datetime(countrywide['사고일시'])\n",
        "\n",
        "# 사고유형이 '차대차', '차대사람', '차량단독'인 데이터만 필터링\n",
        "countrywide = countrywide[countrywide['사고유형'].isin(['차대차', '차대사람', '차량단독'])]\n",
        "\n",
        "countrywide"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train 데이터 가져오기"
      ],
      "metadata": {
        "id": "T_CFOFwvgFDt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI8DNr395Gao"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('./train.csv')\n",
        "\n",
        "# '사고일시' 컬럼을 날짜 형식으로 변환\n",
        "train['사고일시'] = pd.to_datetime(train['사고일시'])\n",
        "\n",
        "train"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 전국구 데이터와 train 데이터 합치기 (total)"
      ],
      "metadata": {
        "id": "Ut-0F0aogOJB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14Y1mQRa5GVt"
      },
      "outputs": [],
      "source": [
        "train_total = pd.concat([train, countrywide], ignore_index=True)\n",
        "\n",
        "# train_total['ECLO'] 열의 null 값 확인\n",
        "null_count = train_total['ECLO'].isna().sum()\n",
        "\n",
        "print(f\"Number of NaN values in 'ECLO': {null_count}\")\n",
        "\n",
        "train_total"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### total 데이터를 사분위수로 이상치 확인하기"
      ],
      "metadata": {
        "id": "u6VlH9i9gS-A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aACggXAw5GTY"
      },
      "outputs": [],
      "source": [
        "# 'ECLO' 열의 상자 그림(boxplot)을 그립니다.\n",
        "train_total['ECLO'].plot(kind='box')\n",
        "plt.title('ECLO Boxplot')\n",
        "plt.ylabel('ECLO Values')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 이상치 제거"
      ],
      "metadata": {
        "id": "08nvifLdgeCp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2Uc4lqZ5GQ6"
      },
      "outputs": [],
      "source": [
        "# 'ECLO' 열의 이상치를 정의하기 위해 사분위수를 계산합니다.\n",
        "Q1 = train_total['ECLO'].quantile(0.25)\n",
        "Q3 = train_total['ECLO'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# 사분위수를 이용하여 이상치의 상한선과 하한선을 정의합니다.\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "\n",
        "# 이상치의 개수를 계산합니다.\n",
        "outliers_index = train_total[(train_total['ECLO'] < lower_bound) | (train_total['ECLO'] > upper_bound)].index\n",
        "outlier_count = outliers_index.shape[0]\n",
        "\n",
        "print(\"이상치 개수:\", outlier_count)\n",
        "\n",
        "# 이상치가 포함된 행을 삭제합니다.\n",
        "train_total.drop(outliers_index, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 다시 이상치 확인"
      ],
      "metadata": {
        "id": "f4ygCS8Bgf4G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foiwbQDu5GOm"
      },
      "outputs": [],
      "source": [
        "# 'ECLO' 열의 상자 그림(boxplot)을 그립니다.\n",
        "train_total['ECLO'].plot(kind='box')\n",
        "plt.title('ECLO Boxplot')\n",
        "plt.ylabel('ECLO Values')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 이상치 제거"
      ],
      "metadata": {
        "id": "beg_DajlgiJt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNx4i8-N5GMI"
      },
      "outputs": [],
      "source": [
        "# 'ECLO' 열의 이상치를 정의하기 위해 사분위수를 계산합니다.\n",
        "Q1 = train_total['ECLO'].quantile(0.25)\n",
        "Q3 = train_total['ECLO'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# 사분위수를 이용하여 이상치의 상한선과 하한선을 정의합니다.\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "\n",
        "# 이상치의 개수를 계산합니다.\n",
        "outliers_index = train_total[(train_total['ECLO'] < lower_bound) | (train_total['ECLO'] > upper_bound)].index\n",
        "outlier_count = outliers_index.shape[0]\n",
        "\n",
        "print(\"이상치 개수:\", outlier_count)\n",
        "\n",
        "# 이상치가 포함된 행을 삭제합니다.\n",
        "train_total.drop(outliers_index, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 이상치 데이터 제거 완료"
      ],
      "metadata": {
        "id": "DoexEVAlgjso"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL9kQJaw5GJi"
      },
      "outputs": [],
      "source": [
        "# 'ECLO' 열의 상자 그림(boxplot)을 그립니다.\n",
        "train_total['ECLO'].plot(kind='box')\n",
        "plt.title('ECLO Boxplot')\n",
        "plt.ylabel('ECLO Values')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 사고유형별 평균 ECLO값이 높은 데이터를 레이블 인코딩 (내림차순)"
      ],
      "metadata": {
        "id": "a_PexEOegs9_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4vZHMZy5k1v"
      },
      "outputs": [],
      "source": [
        "# train_total 데이터프레임에서 '사고유형'별 'ECLO'의 평균 계산\n",
        "mean_eclo_by_type = train_total.groupby('사고유형')['ECLO'].mean().sort_values(ascending=False)\n",
        "\n",
        "# 레이블 인코딩을 위한 매핑 딕셔너리 생성\n",
        "label_mapping = {accident_type: label for label, accident_type in enumerate(mean_eclo_by_type.index)}\n",
        "\n",
        "# '사고유형' 칼럼을 레이블 인코딩하여 새로운 칼럼 추가\n",
        "train_total['사고유형_label'] = train_total['사고유형'].map(label_mapping)\n",
        "\n",
        "# 레이블 인코딩 결과 출력\n",
        "print(\"레이블 인코딩 결과:\")\n",
        "print(train_total[['사고유형', '사고유형_label']].drop_duplicates().sort_values(by='사고유형_label'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### total 데이터의 도로형태 컬럼 변환"
      ],
      "metadata": {
        "id": "_ZjLtm-9hB5W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZH2it5515kzb"
      },
      "outputs": [],
      "source": [
        "# '-'를 기준으로 문자열을 분리하여 새로운 컬럼을 만듭니다.\n",
        "train_total[['도로형태1', '도로형태2']] = train_total['도로형태'].str.split(' - ', expand=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 도로형태별 평균 ECLO값이 높은 데이터를 레이블 인코딩 (내림차순)"
      ],
      "metadata": {
        "id": "ggCCpA8ahgg4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MotspCia5kw-"
      },
      "outputs": [],
      "source": [
        "# 도로형태1에 대한 레이블 인코딩 수행\n",
        "train_total['도로형태1_label'] = label_encoder.fit_transform(train_total['도로형태1'])\n",
        "\n",
        "# 도로형태1_label 값별로 그룹을 형성하여 도로형태2를 레이블 인코딩\n",
        "for label_value in train_total['도로형태1_label'].unique():\n",
        "    # 해당 도로형태1_label 값과 일치하는 행 추출\n",
        "    mask = train_total['도로형태1_label'] == label_value\n",
        "    # 해당 그룹 내의 ECLO 컬럼의 평균값 계산\n",
        "    mean_eclo = train_total.loc[mask, 'ECLO'].mean()\n",
        "    # 해당 그룹 내의 도로형태2를 평균값이 높은 순서대로 숫자 지정하여 레이블 인코딩\n",
        "    label_dict = {address: idx + 1 for idx, address in enumerate(train_total.loc[mask].sort_values(by='ECLO', ascending=False)['도로형태2'].unique())}\n",
        "    train_total.loc[mask, '도로형태2_label'] = train_total.loc[mask, '도로형태2'].map(label_dict)\n",
        "\n",
        "# 도로형태2_label을 정수형으로 변환\n",
        "train_total['도로형태2_label'] = train_total['도로형태2_label'].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test데이터 가져오기"
      ],
      "metadata": {
        "id": "mJoI2tERhnFb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmvEzADM5kuP"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv('./test.csv')\n",
        "test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 사고유형별 평균 ECLO값이 높은 데이터를 레이블 인코딩 (내림차순)"
      ],
      "metadata": {
        "id": "IXXwAFVZjrPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_total 데이터로부터 사고유형에 대한 레이블 인코딩 매핑 생성\n",
        "accident_type_mapping = {}\n",
        "for idx, acc_type in enumerate(train_total['사고유형'].unique()):\n",
        "    accident_type_mapping[acc_type] = idx\n",
        "\n",
        "# train_total 데이터에 사고유형_label 컬럼 추가\n",
        "train_total['사고유형_label'] = train_total['사고유형'].map(accident_type_mapping)\n",
        "\n",
        "# test 데이터에 사고유형_label 컬럼 추가\n",
        "test['사고유형_label'] = test['사고유형'].map(lambda x: accident_type_mapping.get(x, -1))"
      ],
      "metadata": {
        "id": "oKUNlYLDjro3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test의 도로형태 데이터 변환"
      ],
      "metadata": {
        "id": "KXISpKeliYvG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-S27-6565GE4"
      },
      "outputs": [],
      "source": [
        "# '-'를 기준으로 문자열을 분리하여 새로운 컬럼을 만듭니다.\n",
        "test[['도로형태1', '도로형태2']] = test['도로형태'].str.split(' - ', expand=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 도로형태별 평균 ECLO값이 높은 데이터를 레이블 인코딩 (내림차순)"
      ],
      "metadata": {
        "id": "Wy5hMpI5jZxn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZlSuXYhhzCQ"
      },
      "outputs": [],
      "source": [
        "# test 데이터프레임에 '도로형태1_label'과 '도로형태2_label' 컬럼을 추가합니다.\n",
        "test['도로형태1_label'] = None\n",
        "test['도로형태2_label'] = None\n",
        "\n",
        "# train 데이터프레임에서 '도로형태1'과 '도로형태1_label' 사이의 매핑을 딕셔너리 형태로 생성합니다.\n",
        "address1_mapping = dict(zip(train_total['도로형태1'], train_total['도로형태1_label']))\n",
        "\n",
        "# train 데이터프레임에서 '도로형태2'와 '도로형태2_label' 사이의 매핑을 딕셔너리 형태로 생성합니다.\n",
        "address2_mapping = dict(zip(train_total['도로형태2'], train_total['도로형태2_label']))\n",
        "\n",
        "# test 데이터프레임의 각 행에 대해 반복하여 매핑된 값을 할당합니다.\n",
        "for i, row in test.iterrows():\n",
        "    # 도로형태1 train 데이터프레임에 있는 경우 해당하는 도로형태1_label 값을 할당합니다.\n",
        "    if row['도로형태1'] in address1_mapping:\n",
        "        test.at[i, '도로형태1_label'] = address1_mapping[row['도로형태1']]\n",
        "    # 도로형태2가 train 데이터프레임에 있는 경우 해당하는 도로형태2_label 값을 할당합니다.\n",
        "    if row['도로형태2'] in address2_mapping:\n",
        "        test.at[i, '도로형태2_label'] = address2_mapping[row['도로형태2']]\n",
        "\n",
        "test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Catboost의 회귀 모델을 만들고 교차검증을 통해 학습하기(약 10분 소요)"
      ],
      "metadata": {
        "id": "Mu-_jv6ajxP5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfbEkcHDhy_h"
      },
      "outputs": [],
      "source": [
        "# 모델1에 대한 데이터셋\n",
        "X1 = train_total[['도로형태1_label', '도로형태2_label']]\n",
        "y1 = train_total['ECLO']\n",
        "\n",
        "# 모델2에 대한 데이터셋\n",
        "X2 = train_total[['사고유형_label']]\n",
        "y2 = train_total['ECLO']\n",
        "\n",
        "# 교차 검증을 위한 KFold 객체 생성 (6-겹 교차검증, 랜덤 시드 고정)\n",
        "kf = KFold(n_splits=6, shuffle=True, random_state=42)\n",
        "\n",
        "# 모델1에 대한 교차 검증을 수행하고 평균 성능을 출력\n",
        "cv_scores1 = []\n",
        "for train_index, val_index in kf.split(X1):\n",
        "    X_fold_train, X_fold_val = X1.iloc[train_index], X1.iloc[val_index]\n",
        "    y_fold_train, y_fold_val = y1.iloc[train_index], y1.iloc[val_index]\n",
        "\n",
        "    model1 = CatBoostRegressor(random_state=42, verbose=0)\n",
        "    model1.fit(X_fold_train, y_fold_train)\n",
        "    y_pred = model1.predict(X_fold_val)\n",
        "    cv_scores1.append(mean_squared_error(y_fold_val, y_pred))\n",
        "\n",
        "print(\"모델1 교차 검증 결과:\")\n",
        "print(cv_scores1)\n",
        "print(\"모델1 평균 교차 검증 MSE:\", np.mean(cv_scores1))\n",
        "\n",
        "# 모델2에 대한 교차 검증을 수행하고 평균 성능을 출력\n",
        "cv_scores2 = []\n",
        "for train_index, val_index in kf.split(X2):\n",
        "    X_fold_train, X_fold_val = X2.iloc[train_index], X2.iloc[val_index]\n",
        "    y_fold_train, y_fold_val = y2.iloc[train_index], y2.iloc[val_index]\n",
        "\n",
        "    model2 = CatBoostRegressor(random_state=42, verbose=0)\n",
        "    model2.fit(X_fold_train, y_fold_train)\n",
        "    y_pred = model2.predict(X_fold_val)\n",
        "    cv_scores2.append(mean_squared_error(y_fold_val, y_pred))\n",
        "\n",
        "print(\"모델2 교차 검증 결과:\")\n",
        "print(cv_scores2)\n",
        "print(\"모델2 평균 교차 검증 MSE:\", np.mean(cv_scores2))\n",
        "\n",
        "# 새로운 특성 생성\n",
        "X3 = np.column_stack((model1.predict(X1), model2.predict(X2)))\n",
        "\n",
        "# 새로운 모델 정의\n",
        "model_final = CatBoostRegressor(random_state=42, verbose=0)\n",
        "\n",
        "# 모델 final에 대한 교차 검증을 수행하고 평균 성능을 출력\n",
        "cv_scores_final = []\n",
        "for train_index, val_index in kf.split(X3):\n",
        "    X_fold_train, X_fold_val = X3[train_index], X3[val_index]\n",
        "    y_fold_train, y_fold_val = y1.iloc[train_index], y1.iloc[val_index]\n",
        "\n",
        "    model_final.fit(X_fold_train, y_fold_train)\n",
        "    y_pred = model_final.predict(X_fold_val)\n",
        "    cv_scores_final.append(mean_squared_error(y_fold_val, y_pred))\n",
        "\n",
        "print(\"모델 final 교차 검증 결과:\")\n",
        "print(cv_scores_final)\n",
        "print(\"모델 final 평균 교차 검증 MSE:\", np.mean(cv_scores_final))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "e5VdAl65kAYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test의 도로형태, 사고유형별로 변수 생성"
      ],
      "metadata": {
        "id": "xiuIMF3XkPFp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhC7AuSWjgM7"
      },
      "outputs": [],
      "source": [
        "# 테스트 데이터에 대한 예측 생성\n",
        "X1_test = test[['도로형태1_label', '도로형태2_label']]\n",
        "X2_test = test[['사고유형_label']]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델에 test의 도로형태 데이터를 넣어 ECLO값 예측하기(model1)"
      ],
      "metadata": {
        "id": "zQtzM4w5kNQe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVVuaPGZjgKd"
      },
      "outputs": [],
      "source": [
        "# 테스트 데이터에 대한 예측 수행\n",
        "y_pred_test = model1.predict(X1_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델에 test의 사고유형 데이터를 넣어 ECLO값 예측하기(model2)"
      ],
      "metadata": {
        "id": "NthidoSDkWcY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Vu15Nz1jgII"
      },
      "outputs": [],
      "source": [
        "# 테스트 데이터에 대한 예측 수행\n",
        "y_pred_test = model2.predict(X2_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### model1과 model2를 통해 만들어진 모델로 다시 학습시키기(model_final)"
      ],
      "metadata": {
        "id": "7I3IiV3yklG-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HN0cg00SjgF9"
      },
      "outputs": [],
      "source": [
        "# 모델1과 모델2의 예측값을 특성으로 사용하여 새로운 특성 생성\n",
        "X3_test = np.column_stack((model1.predict(X1_test), model2.predict(X2_test)))\n",
        "\n",
        "# 테스트 데이터에 대한 예측 수행\n",
        "y_pred_test = model_final.predict(X3_test)\n",
        "\n",
        "# 테스트 예측값 출력\n",
        "print(\"테스트 예측값:\", y_pred_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sample_submission파일 가져와 예측값 저장"
      ],
      "metadata": {
        "id": "22ht0hf2kzy0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0vSqV7ajn6D"
      },
      "outputs": [],
      "source": [
        "submission = pd.read_csv('./sample_submission.csv')\n",
        "submission['ECLO'] = y_pred_test\n",
        "\n",
        "submission"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 예측값 csv파일로 출력"
      ],
      "metadata": {
        "id": "pJ3M7yjWk636"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akUWXmNj5ArM"
      },
      "outputs": [],
      "source": [
        "submission.to_csv('./submission.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}